# Models

Configure AI models for your agents and tools.

---

## Overview

The Agent Platform supports a wide range of AI models from commercial providers, open-source repositories, and custom fine-tuned deployments. Choose models based on your requirements for accuracy, speed, cost, and capabilities.

---

## Supported Providers

### Commercial Models

| Provider | Models | Tool Calling |
|----------|--------|--------------|
| **OpenAI** | GPT-4, GPT-4o, GPT-4 Turbo, GPT-3.5-turbo | Yes |
| **Anthropic** | Claude 3 Opus, Sonnet, Haiku, Claude 3.5 Sonnet | Yes |
| **Google** | Gemini 1.5 Pro, Gemini 1.5 Flash | Yes |
| **Azure OpenAI** | GPT-4, GPT-3.5-turbo | Yes |

### Open Source Models

Access 30+ models from Hugging Face and other repositories:

- Llama 2/3
- Mistral
- Mixtral
- Falcon
- And more

### Custom Models

Deploy your own fine-tuned models:

- Upload model weights
- Configure inference endpoints
- Set custom parameters

---

## Adding a Model

### Connect to Provider

1. Navigate to **Models** → **Add Connection**
2. Select your provider
3. Enter credentials:

```yaml
# OpenAI
provider: openai
api_key: sk-xxxxxxxxxxxxx

# Azure OpenAI
provider: azure_openai
api_key: xxxxxxxxxxxxx
endpoint: https://your-resource.openai.azure.com/
deployment_name: gpt-4

# Anthropic
provider: anthropic
api_key: sk-ant-xxxxxxxxxxxxx
```

4. Test the connection
5. Save

### Select Model

After connecting, available models appear in dropdowns:

- Agent configuration
- AI nodes in workflows
- Prompt Studio

---

## Model Selection Guide

### By Use Case

| Use Case | Recommended | Why |
|----------|-------------|-----|
| Complex reasoning | GPT-4, Claude 3 Opus | Highest accuracy |
| Fast responses | GPT-3.5, Claude 3 Haiku | Low latency |
| Code generation | GPT-4, Claude 3.5 Sonnet | Best code quality |
| Cost-sensitive | GPT-3.5, Gemini Flash | Lower token cost |
| Long context | Claude 3, Gemini 1.5 | 100K+ tokens |

### By Capabilities

| Capability | Models |
|------------|--------|
| Tool calling | GPT-4, Claude 3, Gemini 1.5 |
| Vision | GPT-4V, Claude 3, Gemini |
| Audio | Whisper (transcription) |
| Embeddings | text-embedding-3, Cohere |

---

## Model Parameters

Configure generation behavior:

### Temperature

Controls randomness (0.0 - 2.0):

```yaml
temperature: 0.0  # Deterministic, focused
temperature: 0.7  # Balanced (default)
temperature: 1.0  # Creative, varied
```

### Top P (Nucleus Sampling)

Limits token selection to probability mass:

```yaml
top_p: 0.9  # Consider tokens in top 90% probability
```

### Top K

Limits token selection to top K options:

```yaml
top_k: 50  # Consider only top 50 tokens
```

### Max Tokens

Maximum output length:

```yaml
max_tokens: 1000  # Limit response to 1000 tokens
```

### Stop Sequences

Strings that stop generation:

```yaml
stop_sequences:
  - "END"
  - "---"
  - "\n\n"
```

---

## Tool Calling Requirements

Not all models support tool calling. For agentic apps:

### Supported

- OpenAI: GPT-4, GPT-4o, GPT-3.5-turbo
- Anthropic: Claude 3 series, Claude 3.5 Sonnet
- Google: Gemini 1.5 Pro, Gemini 1.5 Flash
- Azure: GPT-4, GPT-3.5-turbo

### Not Supported

- Platform-hosted open-source models
- Most Hugging Face models
- Older model versions

---

## Fine-Tuning

Customize models with your data:

### When to Fine-Tune

- Consistent output format needed
- Domain-specific terminology
- Unique tone or style
- Improved accuracy for specific tasks

### Process

1. Prepare training data (JSONL format)
2. Upload to Models Studio
3. Configure fine-tuning parameters
4. Monitor training progress
5. Evaluate results
6. Deploy for use

### Training Data Format

```json
{"messages": [{"role": "system", "content": "You are a support agent."}, {"role": "user", "content": "Order status?"}, {"role": "assistant", "content": "I'll check that for you."}]}
{"messages": [{"role": "system", "content": "You are a support agent."}, {"role": "user", "content": "Refund request"}, {"role": "assistant", "content": "I can help with your refund."}]}
```

---

## Cost Optimization

### Token Economics

| Model | Input (per 1K) | Output (per 1K) |
|-------|---------------|-----------------|
| GPT-4 | $0.03 | $0.06 |
| GPT-4o | $0.005 | $0.015 |
| GPT-3.5 | $0.0005 | $0.0015 |
| Claude 3 Opus | $0.015 | $0.075 |
| Claude 3 Sonnet | $0.003 | $0.015 |
| Claude 3 Haiku | $0.00025 | $0.00125 |

*Prices approximate; check provider for current rates.*

### Strategies

**Use smaller models for simple tasks**:
```yaml
# Routing
simple_faq: gpt-3.5-turbo
complex_reasoning: gpt-4o
```

**Optimize prompts**:
- Remove unnecessary context
- Use concise instructions
- Limit output length

**Cache responses**:
- Cache common queries
- Reuse static content

---

## Monitoring

Track model usage:

- Tokens consumed per model
- Response latencies
- Error rates
- Cost per conversation

Access via **Analytics** → **Model Runs**.

---

## Fallback Configuration

Handle model failures gracefully:

```yaml
primary_model: gpt-4o
fallback_model: gpt-3.5-turbo

fallback_triggers:
  - rate_limit_exceeded
  - timeout
  - server_error

fallback_behavior:
  notify: true
  log: true
```

---

## Related

- [Agents](/agents)
- [Prompt Studio](/prompts)
- [Tool Calling](/tools/calling)
- [Analytics](/analytics)
