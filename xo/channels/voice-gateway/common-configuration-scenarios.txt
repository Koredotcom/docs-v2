# Common Configuration Scenarios in Voice Gateway

Voice Gateway offers several essential configuration options to customize and optimize voice interactions. These configurations enable administrators to fine-tune voice interactions, manage response delays, and create more natural conversational experiences across different languages and use cases.

The common configuration scenarios include:

* Speech engine configuration (ASR/TTS) at global, experience flow, and node levels.
* Continuous ASR for handling digit strings and identifiers.
* Bot delay management for response timing.
* Barge-in controls for handling user interruptions.
* Automated language detection for multilingual support.

## Configure ASR/TTS

Speech Engines (ASR/TTS engines) can be configured at various levels in SmartAssist or the XO Platform.

1. Global level (Channel level)
2. Experience Flow level
3. Node level

**Setting ASR/TTS at the Global level**  

The ASR/TTS engines configured at this level are applicable for all the experience flows (unless a separate speech engine is configured at the experience flow).

Steps to configure ASR/TTS at the global level:

1. Go to Configuration > System Setup > Language & Speech > Voice Preferences.
2. The Voice Preferences window appears. Select the Language from the dropdown.
3. Select the **Automatic Speech Recognition Engine** and the **Dialect** from the dropdowns.
4. Select **Show advanced settings** and select the **Primary** and **Fallback ASR Configurations**.
5. Select the **Text to Speech Engine** and the **Voice** from the dropdowns.
6. Select **Show advanced settings** and select the **Primary** and **Fallback TTS Configurations**.
7. To listen to the voice, enter text into the **Sample Text** dialog box and select **Preview Text**.
8. Select **Done**.  
   <img src="../images/save-voice-preference.png" alt="Save Voice Preference" title="Save Voice Preference" style="border: 1px solid gray; zoom:80%;"/>

**Configure ASR/TTS at the Experience Flow level**

The speech engine configured in an Experience Flow applies to the entire flow and any bot called from within that flow.

Steps to configure ASR/TTS at the Experience Flow Level:

1. Go to Configuration > Experiences > Experience Flows.

2. Select **Edit** on the experience flow to assign a speech engine.  
   <img src="../images/edit-experience-flow.png" alt="Edit Experience Flow" title="Edit Experience Flow" style="border: 1px solid gray; zoom:80%;"/>

3. On the Edit Experience Flow window, select the **Language** from the dropdown.
4. Select the **Automatic Speech Recognition Engine** and the **Dialect** from the dropdowns.
5. Select **Show advanced settings** and select the **Primary** and **Fallback ASR Configurations**.
6. Select the **Text to Speech Engine** and the **Voice** from the dropdowns.
7. Select **Show advanced settings** and select the **Primary** and **Fallback TTS Configurations**.
8. Select **Update & Continue to Flow Design** to save the configurations.  
   <img src="../images/edit-flow-experience.png" alt="Continue to Flow Design" title="Continue to Flow Design" style="border: 1px solid gray; zoom:80%;"/>

**Configure ASR/TTS at a node at the Entity level inside a dialog**  

Speech engines (ASR/TTS) can be configured at any Entity or Message node inside a dialog. When a speech engine is set at an Entity or Message node, it will be used until the end of the call unless a different speech engine is assigned at another Entity or Message node.

The corresponding Call Control Parameter must be set to configure a speech engine at an Entity or Message node.

Steps to set call control parameters:

1. Select the [Entity Node](../../automation/use-cases/dialogs/node-types/working-with-the-entity-node.md)/[Message Node](../../automation/use-cases/dialogs/node-types/working-with-the-message-nodes.md) in the [Dialog Task](../../automation/use-cases/dialogs/node-types/working-with-the-dialog-node.md) where the ASR/TTS will be configured. 
2. Select **Voice Call Properties** and then select **Advanced Controls**.
3. Select **Add** in the Call Control Parameters section to add a call control parameter.
4. Enter the **Parameter Name** and **Value** and select **Confirm** to add the parameter.
5. The parameter is added successfully.

### Continuous ASR

Continuous ASR (Automatic Speech Recognition) is a feature that lets Speech-to-Text (STT) recognition to be tuned for the collection of things like phone numbers, customer identifiers, and other strings of digits or characters, which, when spoken, are often spoken with pauses in between utterances. Two parameters to enable it are:

| Parameter                   | Type                                   | Supporting STT/TTS                         | Description |
|-----------------------------|----------------------------------------|--------------------------------------------|-------------|
| continuousASRTimeoutInMS    | Number (milliseconds). Example: **5000** = 5 seconds | STT: Google, Microsoft<br>TTS: Not required | Specifies the duration of silence, in milliseconds, that the system waits after receiving a transcript from the STT vendor before returning the result. If another transcript arrives before the timeout expires, the system combines the transcripts and continues recognition. The system returns the combined transcript when the silence between utterances exceeds this value. |
| continuousASRDigits         | Any digit or symbol. Examples: **\***, **%**, **&**, **;**, **#** | STT: Google, Microsoft<br>TTS: Not required | Specifies a DTMF key that, when entered, terminates the gather operation and immediately returns the collected results. |

### Handling Bot Delay

If the bot takes time to respond to a message, you can configure Voice Gateway to take action.

### Handle Bot Delay After User Input

The delay is only applied when Voice Gateway sends a response to the bot and is waiting for the bot's reply. This includes delays at the Entity Node, Confirmation Node, or Message Node with an `On Intent` (User-Bot delay).  
<img src="../images/handle-bot-delay.png" alt="Handle Bot Delay" title="Handle Bot Delay" style="border: 1px solid gray; zoom:80%;"/>

If a delay occurs between two Message nodes, the bot developer must handle it manually by playing audio and stopping it after the delay.

By setting timeout properties, the following actions can be configured:

* Play a textual prompt to the user
* Play an audio file to the user
* Disconnect the call

**Use Case**:

* To play a message to the user, configure a timeout on the botNoInputTimeoutMS parameter and define the action:
* To play a textual prompt, set the prompt on the botNoInputSpeech parameter.
* To play an audio file, set the file URL using the botNoInputUrl parameter.
* To replay the message if the timeout is exceeded multiple times, configure the number of retries using the botNoInputRetries parameter.
* A separate timeout for disconnecting the call can be configured using the botNoInputGiveUpTimeoutMS parameter, which is set to 30 seconds by default.

**Parameters description**

The following table lists the bot parameters that are used to configure this feature:

| Parameter                  | Type          | Description | Required |
|----------------------------|---------------|-------------|----------|
| botNoInputGiveUpTimeoutMS  | Number        | Specifies the timeout, in milliseconds, for receiving a bot response before the system disconnects the call. If the bot doesn't respond before the timeout expires, Voice Gateway disconnects the call. The default value is **30,000 ms (30 seconds)**. | Yes (Default: 30 seconds) |
| botNoInputTimeoutMS        | Number        | Specifies the timeout, in milliseconds, before the system plays a prompt to the user when no bot input is received. When the timeout expires, Voice Gateway plays either a textual prompt (configured by **botNoInputSpeech**) or an audio file (configured by **botNoInputUrl**). | Yes |
| botNoInputRetries          | Number        | Specifies the maximum number of retry attempts for bot response timeouts configured by **botNoInputTimeoutMS**. After each timeout, Voice Gateway plays the no-input prompt and restarts the timer. For example, if the value is **2** and the timeout is **1000 ms**, the system plays the prompt up to two times when no bot response is received. | Yes |
| botNoInputSpeech           | String or Array | Specifies the textual prompt played to the user when no bot input is received before the timeout expires. The prompt supports plain text or SSML format. This parameter can include multiple messages and audio URLs. Example: `["https://audiourl", "This is the second message"]`. | Yes |
| botNoInputUrl              | String        | Specifies the URL of the audio file played to the user when no bot input is received before the timeout expires. | Yes |

!!! Note

      `botNoInputSpeech` can contain multiple messages, including audio URLs.

Example: `botNoInputSpeech` = [“this is first delay Msg”, “[https://](https://this)dummy.wav”,” this is third textual Message”].

### Handle Delay Between Two Message Nodes

Voice Gateway can only handle delays when it sends a response to the bot and waits for the bot's reply. If a delay occurs, Voice Gateway can handle it. If a delay occurs between a Message node or Script node where the user hasn’t spoken, Voice Gateway won’t be aware of the delay, and the bot developer must handle it manually.

Adding a Service Node between two Message nodes (delay observed between two Message nodes):
This must be managed manually, as the gateway has already received a command to play a message and isn't waiting for user input. The gateway won't initiate a delay timer and waits for the next bot message.

To handle this scenario:

   * Play music before the API call in the Message node.
   * Configure the Service node.
   * Deliver the message after the Service node.

!!! Note

      If you receive a response from the API and don’t want to play the full music, immediately abort the music and play the Message node prompt using the channel override utility function:

   `print(voiceUtils.abortPrompt(“Dummy message“))` → (The message parameter is optional).  
      <img src="../images/optional-message-parameter.png" alt="Optional Message Parameter" title="Optional Message Parameter" style="border: 1px solid gray; zoom:80%;"/>

### Barge-In

The Barge-In feature controls KoreVG behavior in scenarios where the user starts speaking or dials DTMF digits while the bot is playing its response to the user. In other words, the user interrupts ("barges-in") the bot.

| Parameter                       | Type                              | Supporting STT/TTS                          | Description |
|---------------------------------|-----------------------------------|---------------------------------------------|-------------|
| listenDuringPrompt              | Boolean (true or false)           | STT: Google, Microsoft<br>TTS: Not required | Controls whether the system listens for user speech while the bot plays a prompt. When set to **false**, the system starts listening only after prompt playback completes. The default value is **true**. |
| bargeInMinWordCount             | Number                            | STT: Google, Microsoft<br>TTS: Not required | Specifies the minimum number of words a user must speak to interrupt (barge in on) prompt playback when barge-in is enabled. The default value is **1**. |
| bargeInOnDTMF                   | Boolean                           | STT: Google, Microsoft<br>TTS: Not required | Enables DTMF-based barge-in. When the caller presses any DTMF key, the system stops audio playback and allows the caller to provide input using DTMF or speech. |
| dtmfCollectInterDigitTimeoutMS  | Number (milliseconds)             | STT: Google, Microsoft<br>TTS: Not required | Specifies the timeout, in milliseconds, that Voice Gateway waits for the next DTMF digit before sending the collected digits to the bot. |
| dtmfCollectSubmitDigit          | Number                            | STT: Google, Microsoft<br>TTS: Not required | Specifies a special DTMF submit digit. When the user enters this digit, the system immediately sends all collected digits to the bot without waiting for the timeout or the maximum digit limit. |
| dtmfCollectMaxDigits            | Number                            | STT: Google, Microsoft<br>TTS: Not required | Specifies the maximum number of DTMF digits the system collects. For example, if the value is **5** and the user enters **1234567**, the system sends only **12345** to the bot. |
| dtmfCollectminDigits            | Number                            | STT: Google, Microsoft<br>TTS: Not required | Specifies the minimum number of DTMF digits the system must collect before submitting input. The default value is **1**. |
| dtmfCollectnumDigits            | Number                            | STT: Google, Microsoft<br>TTS: Not required | Specifies the exact number of DTMF digits the system must collect before submitting input. |
| input                           | Array of strings                  | STT: Google, Microsoft<br>TTS: Not required | Specifies the allowed input types. Supported values are **['digits']**, **['speech']**, or **['digits', 'speech']**. The default value is **['digits']**. |


### Language Detection

In this setup, developers don't need to use DTMF or other methods to switch the bot's language. Instead, the bot automatically detects the language based on the user's utterance.

For example, if a user speaks in English, the conversation continues in English. If the user switches to Spanish, the language switches to Spanish. [Learn more](../../app-settings/language-management/managing-languages-for-multilingual-vas.md#adding-a-language-to-a-virtual-assistant).

**Configuration Steps**:

1. In Bot Builder (on the child bot), navigate to **Languages,** add a new language (for example, Spanish), and enable it.
2. Select English as the default language from the language dropdown menu.
3. Create a new dialog titled "Language Detection" (or choose a suitable name).
4. Inside this dialog, add an **entity node** to capture user intent input.
5. Set the entity precedence to **'Intent over Entity'** in the advanced controls.
6. Add the **AlternativeLanguage** call control parameter.
7. Switching languages mid-conversation isn't supported; doing so can cause the bot to lose context. Language detection should happen at the beginning of the conversation (for example, in the welcome task), with the switch based on the user's first utterance.
8. Opt for **'Intent over Entity'** to prioritize intent detection in the user's language.
9. Create another dialog with a specific intent (for example, "book flight") and add relevant entities (for example, selecting source and destination).
10. In the entity configuration, include the following call parameters:
    * **Name**: alternativeLanguages
    * **Value**: [] (Leave it empty if no further language switching is needed).
11. Add utterances in the desired language and train the bot.
12. Change the language to Spanish in the bot language dropdown.
13. Open the intent and update utterances and intent details in Spanish.
14. Update entity details in Spanish as well.
15. Publish the bot.

These steps ensure the bot can detect the user's language at the start and adjust the conversation flow accordingly.