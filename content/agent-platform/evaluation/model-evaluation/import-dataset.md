# Import Datasets

Evaluation Studio provides a flexible approach for importing and managing datasets, making it easier to evaluate model outputs. There are several ways to import and handle datasets, enabling flexibility in evaluating model outputs.

Here's how you can bring data into the platform:

1. **Import a dataset**: Users can upload datasets in CSV format. These datasets may contain:

    * **Input-output pairs** (where each input has a corresponding output)
    * **Input data only** (where outputs need to be generated by a user-defined model)

    Evaluation Studio supports three scenarios for handling the datasets:

    * **Scenario 1:** **One Input, One Output**: The simplest scenario where a dataset has one input column and one output column. Users map the input and output variables to run the evaluation, making it easy to evaluate model predictions.
    * **Scenario 2:** **One Input, Multiple Outputs**: In more complex scenarios, one input may predict multiple outputs through different models. The dataset will have one input column and multiple output columns. Users map the input to the corresponding model outputs for evaluation. Also, users can upload ground truth columns to compare responses.
    * **Scenario 3:** **Input Only**: In this scenario, the dataset contains only input data with no output columns. When users have input data but no corresponding outputs, they can generate model outputs using a pre-trained model. The system will automatically create a new output column based on the input, enabling users to evaluate the pre-trained model's generation.

2. **Import production data**: Users can import data from real-time deployed models. Evaluation Studio allows specific filters like the date range, source where the model is deployed, and columns from the model traces. This helps evaluate your deployed model’s generations. For example, users can identify outputs that took longer to generate or consumed more tokens, gaining insights to achieve an optimal balance between output quality and resource consumption, such as time and token usage.


## Adding Datasets to an Evaluation

Users can add datasets to evaluations within a project. Each dataset represents a collection of inputs and outputs for a specific use case.

Steps to import a dataset:

1. Navigate to Evaluation Studio.
2. Click the Projects tab, and choose the relevant project.
3. Select the specific evaluation to which you want to add datasets.
4. Choose one of the following import methods to import the dataset for evaluation:  
    <img src="../../images/import_dataset_new.png" alt="Upload file" title="Upload file" style="border: 1px solid gray; zoom:80%;">  

    1. **Upload from device**: Click the Upload file link and select your CSV file saved on your local machine.
    2. **Import production data**: Click Proceed and fill in the required fields in the Import production data dialog:
        1. **Models**: Choose the model deployed in production (open-source or commercial). You can select any model used in the Platform within Tools, Prompts, and endpoints. Only data related to the selected model will be retrieved from Model Traces.
        2. **Source**: Select the specific source where the model is deployed, such as Tools, Prompts, or endpoints. You can also select the ‘All’ option to import data from all available sources or specify individual sources like specific prompts or tools. For example, you can select a specific tool to see how the model is performing within that tool.
        3. **Date**: Set the desired date range for the data you want to import. By default, the last 30 days are selected.
        4. **Columns**: The system automatically fetches the input and output columns by default. If you need more detailed analysis, you can select additional columns such as request ID, input tokens, response time, and other relevant metrics. The selected columns will appear in the evaluation table.  



5. Check the preview of the dataset (first 10 rows). To confirm and finalize the import, click **Proceed**.
    <img src="../../images/preview_import_dataset.png" alt="Preview dataset" title="Preview dataset" style="border: 1px solid gray; zoom:80%;">

    The dataset is imported into Evaluation Studio and linked to the selected evaluation. You can then view your data in a tabular format in the evaluation table.
    <img src="../../images/uploaded_dataset_table.png" alt="Evaluation table" title="Evaluation table" style="border: 1px solid gray; zoom:80%;">

6. Click the **+** button on the Evaluations page to access additional dataset actions:
    * **Run a prompt**: Run a prompt by selecting model name and configurations.
    * **Run an API**: Run an API call using specified endpoint and parameters to fetch content from external APIs or deployed tools.
    * **Run Search AI**: Retrieve answers and context chunks using a pre-configured Search AI integration, useful for evaluating retrieval-augmented generation (RAG) systems.
    * **Add an evaluator**: Add a quality or safety evaluator to the dataset.
    * **Add human feedback**: Manually input feedback for model outputs.

    You can also filter the data (text, numeric, boolean), sort it, adjust row heights, and customize columns (hide/show). Filtering is limited to score columns to help with focused analysis. Applied filters and sorting states are clearly indicated and dynamically affect evaluation insights.
    



## Running a Prompt

The Run a Prompt option enables users to generate customized data based on a specific model and prompt. This feature streamlines data creation and enables easy edits and adjustments for continuous improvements.
 
For instance, if you want to replace the manual effort of summarizing customer conversations with a fine-tuned model, you can use Evaluation studio to evaluate its summaries. Start by bringing your conversations as input and deploying the fine-tuned model in the Platform. Then, in Evaluation studio, select 'Run a prompt' and choose your fine-tuned model. In the prompt, you can specify 'summarize the {{input}}' (column as a variable). This variable will capture the conversations, and based on the additional prompt instructions, the model will generate the summary. Finally, you can assign desired evaluators to evaluate the output produced by the fine-tuned model. 
 
**Key Benefits**

* Efficiency: Generate content for multiple categories quickly and easily.
* Customization: Edit prompts and regenerate content to match evolving needs.
* Streamlined Workflow: Manage all content generation tasks in one central place.

**Steps to run a prompt:**

1. On the Evaluations page, click the **+** button and select the **Run a Prompt** option.  
    <img src="../../images/run_a_prompt_option.png" alt="run a prompt" title="Run a prompt" style="border: 1px solid gray; zoom:80%;">

2. In the Run a Prompt dialog:
    1. Enter the **Column Name** for the output data.
    2. Choose the appropriate **Model** and **Configuration** settings.
    3. Type the prompt that describes the data you want to generate, making sure to include any mapped variables.
3. Click **Run** to generate a new output column in your data table with the results.

    <img src="../../images/run_prompt.png" alt="Run a prompt" title="Run a prompt" style="border: 1px solid gray; zoom:80%;">


After running the prompt, the following additional options are available:

* To modify your prompt or configurations, click the **Properties**.
* To refresh the output based on a new prompt or updated data, click **Regenerate**.
* To remove an output column, click **Delete**. Before deleting, ensure that no evaluators are dependent on this column to avoid any errors.

## Running an API

Evaluation Studio offers the ability to run an API, enabling users to fetch content from external APIs or deployed tools directly into their evaluation process. This feature enables the integration of live data or model outputs from deployed agents, providing greater flexibility in the evaluation process.

As a user, you can add a column in Evaluation Studio that triggers an API call to fetch content. This allows you to integrate external data, retrieve agent outputs, and incorporate them into your evaluation. Once the content is fetched, you can evaluate it using human or AI evaluators for in-depth analysis. Using the Run an API feature, you can also fetch outputs from models hosted outside of the Platform. The external model can use the input rows in Evaluation Studio as its input, process the data, and provide the output for each row.

This functionality enhances the evaluation process by providing greater flexibility, allowing users to use external data and models in Evaluation Studio.

**Key Benefits**

* **External Data Integration:** Easily bring in data or model outputs from external sources or deployed tools into your evaluations.
* **Flexible Data Handling:** Allows you to evaluate dynamic content generated by deployed tools in real-time, ensuring up-to-date assessments.
* **Seamless Evaluation:** Attach evaluators to the API-generated outputs just like any other dataset column, ensuring consistency in the evaluation process.
* **Customization:** Tailor the API call and parameters to suit specific needs, enabling flexible and targeted evaluations of external content.

**Steps to run an API:**

1. On the Evaluations page, click the **+** button, and select the **Run an API** option.

    <img src="../../images/run_an_api.png" alt="Run a prompt" title="Run a prompt" style="border: 1px solid gray; zoom:80%;">

2. **Configure the API call**: In the Run an API dialog, specify the following:
    * **Column Name**: Enter a name for the column where the API output will be displayed.
    * **Method**: Select the HTTP method (GET, POST, PUT, DELETE, or PATCH) based on the operation you want to perform with the API.
    * **Request URL**: Enter the URL of the API endpoint. If you have a cURL command for the API request, you can paste it here to test the API call.
    * **Headers**: Define any required headers for the API request. Specify the key/value pairs for the headers, such as authentication tokens or content-type specifications.
    * **Body**: If the request method requires a body (usually for POST or PUT requests), specify the data to be sent in the request. You can use one or more input columns as data in the body. If you want to replace certain parts of the body with dynamic values from input fields, you can use variables for the values. For example, "input":"{{column1}}".
    * **Response**: The response is automatically generated and displayed to show the result of the API call. When testing the API call, the system uses the input from the first row, makes the API request using the provided cURL, and displays the response.
    * **JSON Output Path**: Define the path to the specific data within the JSON response that you want to display. This is useful when the API returns complex JSON data, and you need to extract specific fields or values.

    <img src="../../images/run_an_api_dialog.png" alt="Run a prompt" title="Run a prompt" style="border: 1px solid gray; zoom:80%;">

3. **Test the API call**: Click Test to verify the API setup. The response from the test will be displayed in the Response tab of the properties panel. If the JSON Output Path is incorrect, an error message will appear:
4. **Fetch content from the API:** After configuring the API, click Run to trigger the request. The system will send the API call to the deployed tool, retrieve the output, and automatically add the response as a new column in the evaluation dataset.
5. **Attach evaluators and evaluate the output:** Once the content is added as a column, you can attach evaluators (e.g., Coherence, Toxicity, Bias Detection) to assess the output. Then run the evaluation, and the evaluators will analyze the API-generated data, providing insights into the quality and performance of the content.


### Example Workflow for Running an API

Follow this example to set up and run an API call inside Evaluation Studio:

1. **Create and deploy a tool:** Set up your tool and deploy it in the Platform.
2. **Copy the tool endpoint:** From the **Tool Endpoint** tab, copy the deployed API's URL.  
<img src="../../images/tool_endpoint.png" alt="Copy tool endpoint" title="Copy tool endpoint" style="border: 1px solid gray; zoom:80%;">

3. **Upload a dataset:** In Evaluation Studio, upload a dataset containing only the input columns.
4. **Initiate ‘Run an API’:** Click the **+** button, select **Run an API**, add a column name, and paste the copied endpoint URL in the **Request URL** field.
<img src="../../images/run_api_paste_curl.png" alt="Run an API" title="Run an API" style="border: 1px solid gray; zoom:80%;">

5. **Generate API key:** Go back to the tool, navigate to the **API Keys** tab, create a new key, and copy it.
<img src="../../images/create_api_key.png" alt="Generate API key" title="Generate API key" style="border: 1px solid gray; zoom:80%;">

6. **Set the authorization header:** In Evaluation Studio, in the **Headers** tab, paste the copied API key in the Value field for the Key `x-api-key`.
<img src="../../images/paste_api_key.png" alt="Auth header" title="Auth header" style="border: 1px solid gray; zoom:80%;">

7. **Configure the API body:** Click the **Body** tab. Under the `"input"` key, replace `{{example_text}}` with your input column name. For example, `{{Input}}`.
<img src="../../images/example_text.png" alt="Body tab" title="Body tab" style="border: 1px solid gray; zoom:50%;">

<img src="../../images/test_button.png" alt="Test button" title="Test button" style="border: 1px solid gray; zoom:50%;">


8. **Test the API call and view the response**: Click **Test** to trigger the API. The system uses the first row of your dataset to verify the API setup and displays the response in the **Response** tab. For example, if you are running a summarization tool, you should see the generated summary output based on the input text.  
<img src="../../images/output.summary.png" alt="Results" title="Results" style="border: 1px solid gray; zoom:50%;">

    The **Test** option enables you to preview the JSON response structure. After testing, carefully review the response to identify the correct output path, which you’ll need to specify as the **JSON Output Path** in the following step. 

    <img src="../../images/summary_response.png" alt="Response" title="Response" style="border: 1px solid gray; zoom:70%;">

9. **Define JSON output path:** Click the **JSON output path** tab, and specify the path to extract the required field from the API response.  

    For example, if the API response structure is the following:

    `"output": { "Summarization": "(generated output)" }`

    you should enter output.Summarization as the JSON output path.

    <img src="../../images/output.summary.png" alt="JSON output path" title="JSON output path" style="border: 1px solid gray; zoom:50%;">

10. **Run the API call:** After successful testing, click **Run** to fetch outputs for all the dataset rows. A new column will be added with the populated API responses/results.

    <img src="../../images/populating_responses.png" alt="Results" title="Results" style="border: 1px solid gray; zoom:80%;">


✅ **Tip:** Make sure the column name exactly matches the input column in your dataset to dynamically send each row's input to the API.


## Running Search AI

The Run Search AI option in Evaluation Studio enables users to import evaluation datasets through retrieval-augmented generation (RAG). By leveraging a pre-configured and validated Search AI integration, you can automatically retrieve answers along with supporting context chunks for each input row. This workflow is particularly useful for evaluating RAG systems, knowledge-grounded agents, and other use cases where contextual accuracy is critical.

Once Search AI is executed, Evaluation Studio adds two new columns—Answers and Retrieved Contexts—to the dataset. These outputs can be:

* Evaluated using built-in or custom evaluators,
* Compared against manually entered ground truth data,
* Inspected row by row to assess how well the system retrieves and leverages relevant information.

This functionality helps users test how effectively their RAG system uses external knowledge to generate reliable, grounded responses.

**Key Benefits**

* Import RAG pipelines: Seamlessly integrate your retrieval-augmented (RAG) workflows using only pre-validated Search AI connections for reliable setup and execution.
* Evaluate with custom criteria: Apply custom evaluators to assess the performance and quality of your RAG pipelines based on specific requirements.


**Steps to run Search AI:**

1. **Initiate Search AI connection**: Click the **“+”** icon in Evaluation Studio and select **Run Search AI**.

    <img src="../../images/run_searchai.png" alt="Run Search AI" title="Run Search AI" style="border: 1px solid gray; zoom:80%;">

2. **Select a pre-configured connection**: In the **Connection name** field, select a Search AI connection. Only integrations that are pre-configured and successfully tested via the **Integrations** page will appear in the dropdown. For more information on setting up a Search AI connection, see [Setting up Search AI](../../ai-agents/tools/tool-flows/types-of-nodes/docsearch-node.md#step-1-set-up-search-ai-app).

    <img src="../../images/select_connection.png" alt="Select connection" title="Select connection" style="border: 1px solid gray; zoom:80%;">

3. **Map the input column**: In the **Map Variables** section, specify the input column to use for querying the retrieval system. 

    <img src="../../images/input.png" alt="Map input column" title="Map input column" style="border: 1px solid gray; zoom:80%;">

4. **Apply meta filters (Optional)**: Set Meta filters to define rules that will narrow down the search results. For example, if the sources have multiple files, you can define the specific file names to look up in the meta filters code. [Learn more](https://docs.kore.ai/xo/apis/searchai/answer-generation/#body-parameters). 


5. **Test the connection**: Click **Test** to verify the Search AI connection. The response from the connection will be displayed in the **Response** tab of the properties panel.
The Response tab shows whether the first-row query is returning an answer, allowing the user to verify if the Search AI connection is functioning correctly.

6. **Run Search AI**: Click Run to execute the retrieval process. Evaluation Studio populates two new columns:

    * **Answers**– RAG responses based on the retrieved context.
    * **Retrieved Contexts**– Supporting text chunks used to generate the answer.

    <img src="../../images/answers_ret_contexts.png" alt="Answers and Retrieved Contexts" title="Answers and Retrieved Contexts" style="border: 1px solid gray; zoom:80%;">

    In the **Retrieved Contexts** column, click the **Show JSON** link to open the Response JSON schema. In the opened JSON, retrieved contexts appear under the `chunkText` key. The number of entries under `chunkText` corresponds to the number of retrieved chunks for that row. To understand the context used for a given answer, inspect the values under the `chunkText` key in the response JSON.

    <img src="../../images/show_json.png" alt="JSON chunk text" title="JSON chunk text" style="border: 1px solid gray; zoom:80%;">

7. **Attach Evaluators:** Once the Search AI columns are populated, you can attach any built-in or custom evaluators to the Answers or Retrieved Contexts columns. Click the **“+”** icon and select **Add Evaluator**.  

    !!! note

        The **RAGAS evaluators** are specifically designed to test RAG systems. It is advisable to attach them as evaluators and run an evaluation to thoroughly assess the RAG pipeline's performance. 

    <img src="../../images/ragas.jpg" alt="RAGAS evaluators" title="RAGAS evaluators" style="border: 1px solid gray; zoom:80%;">

    Users can add an empty column, which is inline-editable and supports both text and numeric values, for manually inputting ground truth in RAGAS evaluations.

8. **Review Results**: Navigate through the enriched dataset to inspect Search AI outputs. Use Evaluation Studio’s filtering, sorting, and analysis tools to assess the quality of retrieved contexts and generated answers. You can evaluate how well Search AI retrieves relevant information, how grounded the responses are, and identify opportunities for improvement.



## Key Highlights

* A project can contain multiple evaluations, and users can add a dataset to any evaluation.
* Users can upload a dataset into Evaluation Studio and run evaluations to measure model performance.
* If importing data from production, carefully select the model, source, and date range to ensure you're importing the relevant data.
* Running a prompt enables flexible data generation, allowing users to create customized data based on specific instructions.
* Running an API enables users to integrate live data and model outputs from external APIs or deployed tools, enhancing flexibility in the evaluation process.
* Running Search AI enables users to evaluate RAG systems by retrieving answers and supporting context using validated Search AI integrations.
